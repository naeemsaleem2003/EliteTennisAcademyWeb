{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naeemsaleem2003/EliteTennisAcademyWeb/blob/main/hw1_Naeem_Saleem_(5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bored-graduation",
      "metadata": {
        "id": "bored-graduation"
      },
      "source": [
        "# EECS 487 HW 1 Coding (C) Section: Language Model and Naive Bayes Classifier\n",
        "\n",
        "This notebook contains the coding (C) section of HW 1. In the first problem, you will build two n-gram language models for Yelp reviews, one that operates at the word-level and another that operates on the level of individual characters. You will then use these language models to generate sample Yelp reviews. In the second problem, you will build naive bayes classifiers to distinguish between legitimate news headlines and clickbait.\n",
        "\n",
        "After this assignment, you will learn to\n",
        "1. train ngram language models given a text corpus;\n",
        "2. generate text from a language model;\n",
        "3. calculate probability of some text given a language model;\n",
        "4. classify news headlines using naive bayes classifiers;\n",
        "5. evaluate classifiers by calculating the performance on test set.\n",
        "\n",
        "**Do not edit anything in this python notebook.** All the code you need to write are in ```language_model.py``` and ```naive_bayes.py```.\n",
        "\n",
        "## Setup\n",
        "Run the following cell to upgrade your NLTK version to the latest. You should be using NLTK version >= 3.8 for proper results!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "angry-large",
      "metadata": {
        "id": "angry-large",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5ba49c-1ef0-4a44-9e72-3cb0fb92ae2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.9.1\n"
          ]
        }
      ],
      "source": [
        "!# pip install nltk --upgrade  # after running this line once, you can comment this line out\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "print(nltk.__version__) # make sure this is >=3.8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e6ec2d2",
      "metadata": {
        "id": "6e6ec2d2"
      },
      "source": [
        "Before we get started, run the following cell to load the autoreload extension so that functions in ```language_model.py``` and ```naive_bayes.py``` will be re-imported into the notebook every time we run them. We also need to import all necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "94ca6b2e",
      "metadata": {
        "id": "94ca6b2e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from language_model import *\n",
        "from naive_bayes import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU5vTqf8szlm",
        "outputId": "4fe3e743-ce26-4b3c-d286-d94f3408d52c"
      },
      "id": "oU5vTqf8szlm",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "potential-mason",
      "metadata": {
        "id": "potential-mason"
      },
      "source": [
        "## C.1 N-gram Language Model [40 points]\n",
        "In this problem, you will train two language models on Yelp reviews, one word-level (up to trigrams) and the other character-level (up to 4-grams). The [dataset provided](https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset?resource=download) is a subset of [a larger dataset](https://www.yelp.com/dataset) published by Yelp. We downloaded the file ```yelp.csv``` for you. To begin, you need to first load the data. Here we provide the code for you, but take a look at how we do it because you will need to load the data by yourself later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "enormous-particular",
      "metadata": {
        "id": "enormous-particular",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dac0503-0224-44b5-cdc9-142efaa1a283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trn_text:\n",
            "0       Consistent with the east coast stores, great f...\n",
            "1       I had a serious craving for Roti.  So glad I f...\n",
            "2       Cool atmosphere and good service.\\nThe food is...\n",
            "3       I loved this place! Amazing food and service. ...\n",
            "4       This is the pub burger you have been looking f...\n",
            "                              ...                        \n",
            "7995    Very relaxed atmosphere and employees are supe...\n",
            "7996    07/25/11\\n\\nI have a new crush... and it's Chr...\n",
            "7997    I've been frequenting the Dollar store for the...\n",
            "7998    I love the girls there. I get my eyebrows done...\n",
            "7999    Scale of 1-10 (multiple visits):\\n10 Food\\n9 S...\n",
            "Name: text, Length: 8000, dtype: object\n",
            "\n",
            "dev_text:\n",
            "0       We got here around midnight last Friday... the...\n",
            "1       Brought a friend from Louisiana here.  She say...\n",
            "2       Every friday, my dad and I eat here. We order ...\n",
            "3       My husband and I were really, really disappoin...\n",
            "4       Love this place!  Was in phoenix 3 weeks for w...\n",
            "                              ...                        \n",
            "1995    Recently a friend at the gallery was very exci...\n",
            "1996    If you have never been before you should reall...\n",
            "1997    Consistency every time i return in quality of ...\n",
            "1998    A clean shopping center with many classy optio...\n",
            "1999    What  a beautiful facility. Attention to detai...\n",
            "Name: text, Length: 2000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "filename = 'yelp.csv'\n",
        "df = pd.read_csv(filename)\n",
        "\n",
        "all_text = df['text']\n",
        "\n",
        "trn_text, dev_text = train_test_split(all_text, test_size=0.2, random_state=42)\n",
        "trn_text, dev_text = trn_text.reset_index(drop=True), dev_text.reset_index(drop=True)\n",
        "print(\"trn_text:\")\n",
        "print(trn_text)\n",
        "print(\"\\ndev_text:\")\n",
        "print(dev_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "latest-glance",
      "metadata": {
        "id": "latest-glance"
      },
      "source": [
        "### C.1.1 Data processing and n-gram counts [10 points]\n",
        "Now you need to train your language models on these reviews. You need to implement the class ```NGramLM```. First, **fill in** the function ```get_ngram_counts``` to process the reviews and get the counts of all n-grams in the reviews.\n",
        "\n",
        "Program your `NGramLM` class and `get_ngram_counts` function to collect n-grams of size `n <= ngram_size`. If you refer to the cells below, you will see that `ngram_size = 3` for the word-level model (so collect unigrams, bigrams, and trigrams) and `ngram_size = 4` for the character-level model (so collect unigrams, bigrams, trigrams, and four-grams).\n",
        "\n",
        "Store these n-grams in the dictionary `self.ngram_count`. This dictionary will contain other dictionaries as values. For instance, `self.ngram_count[0]` will be a dictionary containing all of the unigrams, and `self.ngram_count[1]` will be a dictionary containing all the bigrams. To access the count of a unigram, simply use it as a key in the unigram dictionary: ```self.ngram_count[0][\"word1\"]``` will be $C(word1)$. To access the count of a bigram (or trigram or four-gram), simply use a tuple: ```self.ngram_count[1][(\"word1\", \"word2\")]``` will be $C(word1, word2)$.\n",
        "\n",
        "Use the following rules when processing the reviews:\n",
        "- Prepend **two/three** &lt;s&gt; at the beginning of each review as BOS tokens (two for trigram model and three for four-gram model), and append one &lt;/s&gt; at the end of a review as the EOS token.\n",
        "- Convert all letters to lowercase.\n",
        "- Tokenize each review. Use `nltk.tokenize.word_tokenize` for the word-level model and `char_tokenizer` (defined below) for the character-level model. For your convenience, each is passed into NGramLM, so you can program the internals of NGramLM to simply utilize whichever tokenizer is passed in. Do not split BOS and EOS tokens (i.e., even in the character-level model, `<s>` will be a single token rather than three tokens).\n",
        "- Replace all unigrams that occur < 2 times with \"UNK\". For the character-level model, this means that all characters occuring < 2 times should be replaced with UNK. **NOTE:** do this before collecting (n>1)-grams. For instance, if the word \"apple\" appeared < 2 times, then the bigram (\"red\", \"apple\") should instead be (\"red\", \"UNK\").\n",
        "- You will need to make the function ```get_ngram_counts``` flexible enough so that (1) it can operate on both the character-level and the word-level and (2) it can operate on a variety of maximum ngram sizes (as determined by `ngram_size`)\n",
        "- Do **NOT** remove punctuation.\n",
        "\n",
        "Hint: ```collections.defaultdict``` is useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "superb-sympathy",
      "metadata": {
        "id": "superb-sympathy",
        "tags": []
      },
      "outputs": [],
      "source": [
        "bos_token = \"<s>\"\n",
        "eos_token = \"</s>\"\n",
        "\n",
        "class NGramLM:\n",
        "    \"\"\"N-gram language model.\"\"\"\n",
        "\n",
        "    def __init__(self, bos_token, eos_token, tokenizer, ngram_size):\n",
        "        self.ngram_count = {i: defaultdict(int) for i in range(ngram_size)}\n",
        "        self.ngram_size = ngram_size\n",
        "        self.vocab_sum = None  # Could be useful in linear interpolation\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def get_ngram_counts(self, reviews):\n",
        "      unigram_counts = defaultdict(int)\n",
        "\n",
        "      processed_reviews = []\n",
        "      for review in reviews:\n",
        "          tokens = self.tokenizer(review.lower())  # Tokenize and lowercase\n",
        "          tokens = [self.bos_token] * (self.ngram_size - 1) + tokens + [self.eos_token]  # Add BOS/EOS\n",
        "          processed_reviews.append(tokens)\n",
        "\n",
        "          # Count unigrams\n",
        "          for token in tokens:\n",
        "              unigram_counts[token] += 1\n",
        "\n",
        "      # Replace rare unigrams (<2 occurrences) with \"UNK\"\n",
        "      for i, tokens in enumerate(processed_reviews):\n",
        "          processed_reviews[i] = [token if unigram_counts[token] >= 2 else \"UNK\" for token in tokens]\n",
        "\n",
        "      # Second pass: Count n-grams\n",
        "      for tokens in processed_reviews:\n",
        "          for n in range(self.ngram_size):\n",
        "              for i in range(len(tokens) - n):\n",
        "                  ngram = tuple(tokens[i:i + n + 1]) if n > 0 else tokens[i]  # Use tuple for bigram and above\n",
        "                  self.ngram_count[n][ngram] += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef26743",
      "metadata": {
        "id": "3ef26743"
      },
      "source": [
        "Now that you have tested ```get_ngram_counts``` on the word-level model (above), test it on the character-level model below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ethical-giant",
      "metadata": {
        "id": "ethical-giant",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e158b1ed-0dd2-4cf5-9bae-7aa1e60f83f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unigrams: 12\n",
            "Unigram with smallest count: e\tCount: 2\n",
            "Unknown unigram count: 10\n",
            "Number of BOS token: 9\n",
            "Number of bigrams: 32\n"
          ]
        }
      ],
      "source": [
        "def char_tokenizer(text):\n",
        "    return list(text)\n",
        "\n",
        "bos_token = \"<s>\"\n",
        "eos_token = \"</s>\"\n",
        "ngram_size = 4\n",
        "char_lm = NGramLM(bos_token, eos_token, char_tokenizer, ngram_size)\n",
        "\n",
        "# Sample text\n",
        "trn_text = [\"This is a test.\", \"I love coding.\", \"NLP is fun!\"]\n",
        "char_lm.get_ngram_counts(trn_text)\n",
        "print(f\"Number of unigrams: {len(char_lm.ngram_count[0])}\")\n",
        "least_unigram = min(char_lm.ngram_count[0].keys(), key=lambda x: char_lm.ngram_count[0][x])\n",
        "print(f\"Unigram with smallest count: {least_unigram}\\tCount: {char_lm.ngram_count[0][least_unigram]}\")\n",
        "print(f\"Unknown unigram count: {char_lm.ngram_count[0]['UNK']}\")\n",
        "print(f\"Number of BOS token: {char_lm.ngram_count[0][bos_token]}\")\n",
        "print(f\"Number of bigrams: {len(char_lm.ngram_count[1])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "peripheral-charge",
      "metadata": {
        "id": "peripheral-charge"
      },
      "source": [
        "### C.1.2 Add-k Smoothing [5 points]\n",
        "As discussed in the lecture, simply counting the number of occurrence of n-grams will assign 0 probability to n-grams that don't appear in the training corpus and thus cannot generalize to unseen data. To mitigate this, you need to implement some smoothing techniques. **Fill in** the function ```add_k_prob``` so that given a bigram $(w_1, w_2)$, a unigram $w_3$, and $k$, return $p(w_3|(w_1, w_2))$ after applying add-k smoothing.<br>\n",
        "\n",
        "Notes:\n",
        "- Program this flexibly enough so that, for the character-level model, the model can smooth over trigrams and unigrams rather than bigrams and unigrams.\n",
        "- &lt;s&gt; should **NOT** be considered when calculating the vocabulary size because it will never be generated by the language model (although it's in ```self.unigram_count```). &lt;/s&gt; and \"UNK\" should both be treated as tokens in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "substantial-trust",
      "metadata": {
        "id": "substantial-trust",
        "tags": []
      },
      "outputs": [],
      "source": [
        "bos_token = \"<s>\"\n",
        "eos_token = \"</s>\"\n",
        "\n",
        "class NGramLM:\n",
        "    \"\"\"N-gram language model.\"\"\"\n",
        "\n",
        "    def __init__(self, bos_token, eos_token, tokenizer, ngram_size):\n",
        "        self.ngram_count = {i: defaultdict(int) for i in range(ngram_size)}\n",
        "        self.ngram_size = ngram_size\n",
        "        self.vocab_sum = None  # Could be useful in linear interpolation\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def get_ngram_counts(self, reviews):\n",
        "      unigram_counts = defaultdict(int)\n",
        "\n",
        "      processed_reviews = []\n",
        "      for review in reviews:\n",
        "          tokens = self.tokenizer(review.lower())  # Tokenize and lowercase\n",
        "          tokens = [self.bos_token] * (self.ngram_size - 1) + tokens + [self.eos_token]  # Add BOS/EOS\n",
        "          processed_reviews.append(tokens)\n",
        "\n",
        "          # Count unigrams\n",
        "          for token in tokens:\n",
        "              unigram_counts[token] += 1\n",
        "\n",
        "      # Replace rare unigrams (<2 occurrences) with \"UNK\"\n",
        "      for i, tokens in enumerate(processed_reviews):\n",
        "          processed_reviews[i] = [token if unigram_counts[token] >= 2 else \"UNK\" for token in tokens]\n",
        "\n",
        "      # Second pass: Count n-grams\n",
        "      for tokens in processed_reviews:\n",
        "          for n in range(self.ngram_size):\n",
        "              for i in range(len(tokens) - n):\n",
        "                  ngram = tuple(tokens[i:i + n + 1]) if n > 0 else tokens[i]  # Use tuple for bigram and above\n",
        "                  self.ngram_count[n][ngram] += 1\n",
        "\n",
        "    def add_k_smooth_prob(self, n_minus1_gram, unigram, k):\n",
        "      ngram = n_minus1_gram + (unigram,)\n",
        "\n",
        "      ngram_count = self.ngram_count[len(n_minus1_gram)].get(ngram, 0)  # Count of n-gram\n",
        "      context_count = self.ngram_count[len(n_minus1_gram) - 1].get(n_minus1_gram, 0)  # Count of context\n",
        "\n",
        "      # Compute vocabulary size (excluding <s>)\n",
        "      vocabulary_size = len([word for word in self.ngram_count[0].keys() if word != self.bos_token])\n",
        "\n",
        "      probability = (ngram_count + k) / (context_count + k * vocabulary_size) if context_count > 0 else 0\n",
        "\n",
        "      return probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3IyCHiGsLld"
      },
      "source": [
        "Now test ```linear_interp_prob``` on the character-level model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "o3IyCHiGsLld"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "unexpected-status",
      "metadata": {
        "id": "unexpected-status",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4fa251-22d4-4552-c6b5-c4ecd52e95d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['<s>', 'p', 'l', 'a', 'n', '</s>', 't', 'UNK']\n",
            "Probability of seen: 0.5384615384615384\n",
            "Probability of unseen: 0.07692307692307693\n"
          ]
        }
      ],
      "source": [
        "test_data = ['plan', 'plant', 'planet']\n",
        "k = 0.5\n",
        "test_char_lm = NGramLM(bos_token, eos_token, char_tokenizer, 4)\n",
        "test_char_lm.get_ngram_counts(test_data)\n",
        "print(f\"Vocabulary: {list(test_char_lm.ngram_count[0].keys())}\")\n",
        "# test\n",
        "trigram = ('p', 'l', 'a')\n",
        "unigram = 'n'\n",
        "print(f\"Probability of seen: {test_char_lm.add_k_smooth_prob(trigram, unigram, k)}\")\n",
        "\n",
        "trigram = ('p', 'l', 'a')\n",
        "unigram = 't'\n",
        "print(f\"Probability of unseen: {test_char_lm.add_k_smooth_prob(trigram, unigram, k)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yellow-respondent",
      "metadata": {
        "id": "yellow-respondent"
      },
      "source": [
        "### C.1.3 Linear interpolation [4 points]\n",
        "Similarly, **fill in** the function ```linear_interp_prob``` so that given a bigram $(w_1, w_2)$, a unigram $w_3$, and list of values [$\\lambda_1$, $\\lambda_2$, $\\lambda_3$], return $p(w_3|(w_1, w_2))$ after applying linear interpolation.\n",
        "\n",
        "Once again, implement this flexibly enough to operate on four-grams for the character-level model.\n",
        "\n",
        "A couple notes:\n",
        "- In certain instances, you may end up with a fraction 0/0 due to the fact that the count for both the n-gram and (n-1)-gram are equal to zero. In these instances, treat the entire fraction as 0 (even though technically $\\frac{0}{0} \\neq 0$).\n",
        "- `<s>` should **NOT** be included when counting the total number of tokens for the unigram probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "latest-summit",
      "metadata": {
        "id": "latest-summit",
        "tags": []
      },
      "outputs": [],
      "source": [
        "bos_token = \"<s>\"\n",
        "eos_token = \"</s>\"\n",
        "\n",
        "class NGramLM:\n",
        "    \"\"\"N-gram language model.\"\"\"\n",
        "\n",
        "    def __init__(self, bos_token, eos_token, tokenizer, ngram_size):\n",
        "        self.ngram_count = {i: defaultdict(int) for i in range(ngram_size)}\n",
        "        self.ngram_size = ngram_size\n",
        "        self.vocab_sum = None  # Could be useful in linear interpolation\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def get_ngram_counts(self, reviews):\n",
        "      unigram_counts = defaultdict(int)\n",
        "\n",
        "      processed_reviews = []\n",
        "      for review in reviews:\n",
        "          tokens = self.tokenizer(review.lower())  # Tokenize and lowercase\n",
        "          tokens = [self.bos_token] * (self.ngram_size - 1) + tokens + [self.eos_token]  # Add BOS/EOS\n",
        "          processed_reviews.append(tokens)\n",
        "\n",
        "          # Count unigrams\n",
        "          for token in tokens:\n",
        "              unigram_counts[token] += 1\n",
        "\n",
        "      # Replace rare unigrams (<2 occurrences) with \"UNK\"\n",
        "      for i, tokens in enumerate(processed_reviews):\n",
        "          processed_reviews[i] = [token if unigram_counts[token] >= 2 else \"UNK\" for token in tokens]\n",
        "\n",
        "      # Second pass: Count n-grams\n",
        "      for tokens in processed_reviews:\n",
        "          for n in range(self.ngram_size):\n",
        "              for i in range(len(tokens) - n):\n",
        "                  ngram = tuple(tokens[i:i + n + 1]) if n > 0 else tokens[i]  # Use tuple for bigram and above\n",
        "                  self.ngram_count[n][ngram] += 1\n",
        "\n",
        "    def add_k_smooth_prob(self, n_minus1_gram, unigram, k):\n",
        "      ngram = n_minus1_gram + (unigram,)\n",
        "\n",
        "      ngram_count = self.ngram_count[len(n_minus1_gram)].get(ngram, 0)  # Count of n-gram\n",
        "      context_count = self.ngram_count[len(n_minus1_gram) - 1].get(n_minus1_gram, 0)  # Count of context\n",
        "\n",
        "      # Compute vocabulary size (excluding <s>)\n",
        "      vocabulary_size = len([word for word in self.ngram_count[0].keys() if word != self.bos_token])\n",
        "\n",
        "      probability = (ngram_count + k) / (context_count + k * vocabulary_size) if context_count > 0 else 0\n",
        "\n",
        "      return probability\n",
        "\n",
        "    def linear_interp_prob(self, n_minus1_gram, unigram, lambdas):\n",
        "      assert len(lambdas) == len(n_minus1_gram) + 1, \"Lambda count must match n-gram order\"\n",
        "\n",
        "      probability = 0\n",
        "      for i in range(len(lambdas)):\n",
        "          if i == 0:  # Unigram probability\n",
        "              count_ngram = self.ngram_count[0].get(unigram, 0)\n",
        "              total_count = sum(self.ngram_count[0].values()) - self.ngram_count[0].get(self.bos_token, 0)\n",
        "          else:  # Higher n-grams\n",
        "              ngram = tuple(n_minus1_gram[-i:]) + (unigram,)\n",
        "              count_ngram = self.ngram_count[i].get(ngram, 0)\n",
        "              total_count = self.ngram_count[i - 1].get(tuple(n_minus1_gram[-i:]), 0)\n",
        "\n",
        "          level_probability = (count_ngram / total_count) if total_count > 0 else 0\n",
        "          probability += lambdas[i] * level_probability\n",
        "\n",
        "      return probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "accessible-debut",
      "metadata": {
        "id": "accessible-debut",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "52c40b90-0a34-4eb0-f7a0-35c6d6c7ddb8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NGramLM' object has no attribute 'linear_interp_prob'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ad4d0ef45484>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0munigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Probability of seen: {test_char_lm.linear_interp_prob(trigram, unigram, lambdas)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NGramLM' object has no attribute 'linear_interp_prob'"
          ]
        }
      ],
      "source": [
        "lambda1 = 0.6\n",
        "lambda2 = 0.2\n",
        "lambda3 = 0.1\n",
        "lambda4 = 0.1\n",
        "lambdas = [lambda1, lambda2, lambda3, lambda4]\n",
        "\n",
        "trigram = ('p', 'l', 'a')\n",
        "unigram = 'n'\n",
        "print(f\"Probability of seen: {test_char_lm.linear_interp_prob(trigram, unigram, lambdas)}\")\n",
        "\n",
        "trigram = ('p', 'l', 'a')\n",
        "unigram = 't'\n",
        "print(f\"Probability of unseen: {test_char_lm.linear_interp_prob(trigram, unigram, lambdas)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "front-cambridge",
      "metadata": {
        "id": "front-cambridge"
      },
      "source": [
        "### C.1.4 Calculate next word probability [2 points]\n",
        "**Fill in** the function ```get_probability``` that calculates $p(w_3|(w_1, w_2))$ using either add-k smoothing or linear interpolation that you implemented above. The input is a dictionary that specifies how should you do the smoothing. Once again, program this function flexibly so that it works in the character-level model as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trying-delivery",
      "metadata": {
        "id": "trying-delivery",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class NGramLM:\n",
        "    \"\"\"N-gram language model.\"\"\"\n",
        "\n",
        "    def __init__(self, bos_token, eos_token, tokenizer, ngram_size):\n",
        "        self.ngram_count = {i: defaultdict(int) for i in range(ngram_size)}\n",
        "        self.ngram_size = ngram_size\n",
        "        self.vocab_sum = None  # Useful for smoothing\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def get_ngram_counts(self, reviews):\n",
        "        \"\"\"Processes text and collects n-gram counts up to ngram_size.\"\"\"\n",
        "        unigram_counts = defaultdict(int)\n",
        "\n",
        "        # First pass: Tokenize, lowercase, add BOS/EOS, and count unigrams\n",
        "        processed_reviews = []\n",
        "        for review in reviews:\n",
        "            tokens = self.tokenizer(review.lower())  # Tokenize and lowercase\n",
        "            tokens = [self.bos_token] * (self.ngram_size - 1) + tokens + [self.eos_token]  # Add BOS/EOS\n",
        "            processed_reviews.append(tokens)\n",
        "\n",
        "            # Count unigrams\n",
        "            for token in tokens:\n",
        "                unigram_counts[token] += 1\n",
        "\n",
        "        # Replace rare unigrams (<2 occurrences) with \"UNK\"\n",
        "        for i, tokens in enumerate(processed_reviews):\n",
        "            processed_reviews[i] = [token if unigram_counts[token] >= 2 else \"UNK\" for token in tokens]\n",
        "\n",
        "        # Second pass: Count n-grams\n",
        "        for tokens in processed_reviews:\n",
        "            for n in range(self.ngram_size):\n",
        "                for i in range(len(tokens) - n):\n",
        "                    ngram = tuple(tokens[i:i + n + 1]) if n > 0 else tokens[i]  # Use tuple for bigram and above\n",
        "                    self.ngram_count[n][ngram] += 1\n",
        "\n",
        "    def linear_interp_prob(self, n_minus1_gram, unigram, lambdas):\n",
        "        \"\"\"\n",
        "        Compute probability using linear interpolation.\n",
        "        Supports both word and character models.\n",
        "\n",
        "        Args:\n",
        "            n_minus1_gram: The preceding (n-1)-gram (tuple).\n",
        "            unigram: The next token to predict.\n",
        "            lambdas: List of lambda weights.\n",
        "\n",
        "        Returns:\n",
        "            Interpolated probability.\n",
        "        \"\"\"\n",
        "        assert len(lambdas) == len(n_minus1_gram) + 1, \"Lambda count must match n-gram order\"\n",
        "\n",
        "        probability = 0\n",
        "\n",
        "        # Iterate over all n-gram levels (unigram, bigram, trigram, etc.)\n",
        "        for i in range(len(lambdas)):\n",
        "            if i == 0:  # Unigram probability\n",
        "                count_ngram = self.ngram_count[0].get(unigram, 0)\n",
        "                total_count = sum(self.ngram_count[0].values()) - self.ngram_count[0].get(self.bos_token, 0)\n",
        "            else:  # Higher n-grams (bigram, trigram, etc.)\n",
        "                ngram = tuple(n_minus1_gram[-i:]) + (unigram,)\n",
        "                count_ngram = self.ngram_count[i].get(ngram, 0)\n",
        "                total_count = self.ngram_count[i - 1].get(tuple(n_minus1_gram[-i:]), 0)\n",
        "\n",
        "            # Avoid 0/0 errors\n",
        "            level_probability = (count_ngram / total_count) if total_count > 0 else 0\n",
        "\n",
        "            # Apply lambda weight\n",
        "            probability += lambdas[i] * level_probability\n",
        "\n",
        "        return probability\n",
        "\n",
        "    def get_probability(self, n_minus1_gram, unigram, smoothing_args):\n",
        "      \"\"\"\n",
        "      Compute the probability of `unigram` given `n_minus1_gram` using the specified smoothing method.\n",
        "\n",
        "      Args:\n",
        "          n_minus1_gram: The preceding (n-1)-gram (tuple).\n",
        "          unigram: The next token to predict.\n",
        "          smoothing_args: A dictionary specifying the smoothing method and its parameters.\n",
        "\n",
        "      Returns:\n",
        "          Probability of `unigram` given `n_minus1_gram`.\n",
        "      \"\"\"\n",
        "      method = smoothing_args.get('method', 'add_k')  # Default to add-k\n",
        "\n",
        "      if method == 'add_k':\n",
        "          k = smoothing_args.get('k', 1.0)  # Default k value if not provided\n",
        "          return self.add_k_smooth_prob(n_minus1_gram, unigram, k)\n",
        "\n",
        "      elif method == 'linear':\n",
        "          lambdas = smoothing_args.get('lambdas', [0.6, 0.2, 0.2])  # Default lambda values\n",
        "          return self.linear_interp_prob(n_minus1_gram, unigram, lambdas)\n",
        "\n",
        "      else:\n",
        "          raise ValueError(\"Invalid smoothing method. Use 'add_k' or 'linear'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multiple-singer",
      "metadata": {
        "id": "multiple-singer"
      },
      "source": [
        "### C.1.5 Calculate perplexity [4 points]\n",
        "One way to evaluate the language model is to calculate its perplexity on some validation data. **Fill in** the function `get_perplexity` so that, given a document and smoothing arguments, it returns the perplexity of the document. Remember to follow the **same** processing steps you used previously. To avoid numerical underflow, calculate the log of the perplexity first i.e.\n",
        "\n",
        "$$\n",
        "PPL(W) = \\exp\\left(\\log\\left(\\sqrt[N]{\\prod_{i=1}^N{\\frac{1}{p(w_i | w_{i-n+1} \\dots w_{i-1})}}}\\right)\\right) = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^N \\log{p(w_i | w_{i-n+1} \\dots w_{i-1})}\\right)\n",
        "$$\n",
        "\n",
        "where $n$ is the n-gram order (e.g., $n=3$ for the trigram word-level model or $n=4$ for the 4-gram character-level model).  \n",
        "In particular for a trigram model, $w_0 = w_{-1} = \\text{<s>}$ and $w_N = \\text{</s>}$. Remember to program this flexibly enough that it can work for both the word-level and character-level models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hawaiian-examination",
      "metadata": {
        "id": "hawaiian-examination",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class NGramLM:\n",
        "    \"\"\"N-gram language model.\"\"\"\n",
        "\n",
        "    def __init__(self, bos_token, eos_token, tokenizer, ngram_size):\n",
        "        self.ngram_count = {i: defaultdict(int) for i in range(ngram_size)}\n",
        "        self.ngram_size = ngram_size\n",
        "        self.vocab_sum = None  # Useful for smoothing\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def get_ngram_counts(self, reviews):\n",
        "        \"\"\"Processes text and collects n-gram counts up to ngram_size.\"\"\"\n",
        "        unigram_counts = defaultdict(int)\n",
        "\n",
        "        # First pass: Tokenize, lowercase, add BOS/EOS, and count unigrams\n",
        "        processed_reviews = []\n",
        "        for review in reviews:\n",
        "            tokens = self.tokenizer(review.lower())  # Tokenize and lowercase\n",
        "            tokens = [self.bos_token] * (self.ngram_size - 1) + tokens + [self.eos_token]  # Add BOS/EOS\n",
        "            processed_reviews.append(tokens)\n",
        "\n",
        "            # Count unigrams\n",
        "            for token in tokens:\n",
        "                unigram_counts[token] += 1\n",
        "\n",
        "        # Replace rare unigrams (<2 occurrences) with \"UNK\"\n",
        "        for i, tokens in enumerate(processed_reviews):\n",
        "            processed_reviews[i] = [token if unigram_counts[token] >= 2 else \"UNK\" for token in tokens]\n",
        "\n",
        "        # Second pass: Count n-grams\n",
        "        for tokens in processed_reviews:\n",
        "            for n in range(self.ngram_size):\n",
        "                for i in range(len(tokens) - n):\n",
        "                    ngram = tuple(tokens[i:i + n + 1]) if n > 0 else tokens[i]  # Use tuple for bigram and above\n",
        "                    self.ngram_count[n][ngram] += 1\n",
        "\n",
        "    def linear_interp_prob(self, n_minus1_gram, unigram, lambdas):\n",
        "        \"\"\"\n",
        "        Compute probability using linear interpolation.\n",
        "        Supports both word and character models.\n",
        "\n",
        "        Args:\n",
        "            n_minus1_gram: The preceding (n-1)-gram (tuple).\n",
        "            unigram: The next token to predict.\n",
        "            lambdas: List of lambda weights.\n",
        "\n",
        "        Returns:\n",
        "            Interpolated probability.\n",
        "        \"\"\"\n",
        "        assert len(lambdas) == len(n_minus1_gram) + 1, \"Lambda count must match n-gram order\"\n",
        "\n",
        "        probability = 0\n",
        "\n",
        "        # Iterate over all n-gram levels (unigram, bigram, trigram, etc.)\n",
        "        for i in range(len(lambdas)):\n",
        "            if i == 0:  # Unigram probability\n",
        "                count_ngram = self.ngram_count[0].get(unigram, 0)\n",
        "                total_count = sum(self.ngram_count[0].values()) - self.ngram_count[0].get(self.bos_token, 0)\n",
        "            else:  # Higher n-grams (bigram, trigram, etc.)\n",
        "                ngram = tuple(n_minus1_gram[-i:]) + (unigram,)\n",
        "                count_ngram = self.ngram_count[i].get(ngram, 0)\n",
        "                total_count = self.ngram_count[i - 1].get(tuple(n_minus1_gram[-i:]), 0)\n",
        "\n",
        "            # Avoid 0/0 errors\n",
        "            level_probability = (count_ngram / total_count) if total_count > 0 else 0\n",
        "\n",
        "            # Apply lambda weight\n",
        "            probability += lambdas[i] * level_probability\n",
        "\n",
        "        return probability\n",
        "\n",
        "    def get_probability(self, n_minus1_gram, unigram, smoothing_args):\n",
        "      \"\"\"\n",
        "      Compute the probability of `unigram` given `n_minus1_gram` using the specified smoothing method.\n",
        "\n",
        "      Args:\n",
        "          n_minus1_gram: The preceding (n-1)-gram (tuple).\n",
        "          unigram: The next token to predict.\n",
        "          smoothing_args: A dictionary specifying the smoothing method and its parameters.\n",
        "\n",
        "      Returns:\n",
        "          Probability of `unigram` given `n_minus1_gram`.\n",
        "      \"\"\"\n",
        "      method = smoothing_args.get('method', 'add_k')  # Default to add-k\n",
        "\n",
        "      if method == 'add_k':\n",
        "          k = smoothing_args.get('k', 1.0)  # Default k value if not provided\n",
        "          return self.add_k_smooth_prob(n_minus1_gram, unigram, k)\n",
        "\n",
        "      elif method == 'linear':\n",
        "          lambdas = smoothing_args.get('lambdas', [0.6, 0.2, 0.2])  # Default lambda values\n",
        "          return self.linear_interp_prob(n_minus1_gram, unigram, lambdas)\n",
        "\n",
        "      else:\n",
        "          raise ValueError(\"Invalid smoothing method. Use 'add_k' or 'linear'.\")\n",
        "\n",
        "\n",
        "\n",
        "    def get_perplexity(self, text, smoothing_args):\n",
        "      \"\"\"\n",
        "      Compute the perplexity of a given text using the specified smoothing method.\n",
        "\n",
        "      Args:\n",
        "          text: The input text (string) to evaluate.\n",
        "          smoothing_args: A dictionary specifying the smoothing method and its parameters.\n",
        "\n",
        "      Returns:\n",
        "          Perplexity score (float).\n",
        "      \"\"\"\n",
        "      # Tokenize and preprocess the text\n",
        "      tokens = self.tokenizer(text.lower())  # Tokenization and lowercasing\n",
        "      tokens = [self.bos_token] * (self.ngram_size - 1) + tokens + [self.eos_token]  # Add BOS and EOS\n",
        "\n",
        "      log_prob_sum = 0\n",
        "      N = len(tokens) - (self.ngram_size - 1)  # Number of valid predictions\n",
        "\n",
        "      # Iterate over the text using n-grams\n",
        "      for i in range(self.ngram_size - 1, len(tokens)):\n",
        "          n_minus1_gram = tuple(tokens[i - (self.ngram_size - 1):i])  # Context (n-1) words\n",
        "          unigram = tokens[i]  # Next word/character\n",
        "\n",
        "          # Get probability using smoothing method\n",
        "          prob = self.get_probability(n_minus1_gram, unigram, smoothing_args)\n",
        "\n",
        "          # Handle log(0) case: if prob is 0, return high perplexity\n",
        "          if prob == 0:\n",
        "              return float('inf')\n",
        "\n",
        "          log_prob_sum += np.log(prob)\n",
        "\n",
        "      # Compute perplexity\n",
        "      perplexity = np.exp(-log_prob_sum / N)\n",
        "\n",
        "      return perplexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coupled-correlation",
      "metadata": {
        "id": "coupled-correlation"
      },
      "source": [
        "### C.1.6 Search hyperparameters [6 points]\n",
        "Now you are ready to find your best language models! First find the best k value for the word-level model and for the character-level model using add-k smoothing. You need to search k in this list: \\[0.2, 0.4, 0.6, 0.8, 1.0\\].\n",
        "\n",
        "**Fill in** the function ```search_k``` such that given a validation set, return the best k value on it. Print out the perplexity (average on the whole validation set) for each k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acceptable-dealer",
      "metadata": {
        "id": "acceptable-dealer",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def search_k(self, dev_data):\n",
        "    \"\"\"\n",
        "    Search for the best k value for add-k smoothing by testing on the validation set.\n",
        "\n",
        "    Args:\n",
        "        dev_data: Validation dataset (list of text samples).\n",
        "\n",
        "    Returns:\n",
        "        Best k value (float) that minimizes perplexity.\n",
        "    \"\"\"\n",
        "    k_values = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "    best_k = None\n",
        "    lowest_perplexity = float('inf')\n",
        "\n",
        "    print(\"Searching for best k...\")\n",
        "\n",
        "    for k in k_values:\n",
        "        smoothing_args = {'method': 'add_k', 'k': k}\n",
        "        total_perplexity = 0\n",
        "        count = 0\n",
        "\n",
        "        for text in dev_data:\n",
        "            perplexity = self.get_perplexity(text, smoothing_args)\n",
        "            total_perplexity += perplexity\n",
        "            count += 1\n",
        "\n",
        "        avg_perplexity = total_perplexity / count\n",
        "        print(f\"k={k}: Perplexity={avg_perplexity:.3f}\")\n",
        "\n",
        "        if avg_perplexity < lowest_perplexity:\n",
        "            lowest_perplexity = avg_perplexity\n",
        "            best_k = k\n",
        "\n",
        "    return best_k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hydraulic-theorem",
      "metadata": {
        "id": "hydraulic-theorem"
      },
      "source": [
        "Similarly, **fill in** the function ```search_lambda``` such that, given a validation set, returns the best $\\lambda$ values on it. You need to choose the search list by yourself. Print out the best set of $\\lambda$ and corresponding perplexity. To get full credits, your perplexity scores need to be < 180 for the word-level model and < 15 for the character-level model.\n",
        "\n",
        "Note: this code block might take a couple minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blocked-monaco",
      "metadata": {
        "id": "blocked-monaco",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622a55ff-e198-4ebb-f042-08a25e48175e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word LM\n",
            "Evaluating different k values...\n",
            "k = 0.2: Perplexity = 4.999999999999999\n",
            "k = 0.4: Perplexity = 2.5\n",
            "k = 0.6: Perplexity = 1.6666666666666667\n",
            "k = 0.8: Perplexity = 1.25\n",
            "k = 1.0: Perplexity = 1.0\n",
            "Searching best lambda...\n",
            "Lambdas = (0.1, 0.1, 0.8): Perplexity = 1.0\n",
            "Lambdas = (0.1, 0.2, 0.7): Perplexity = 1.0\n",
            "Lambdas = (0.1, 0.3, 0.6): Perplexity = 1.0\n",
            "Lambdas = (0.1, 0.4, 0.5): Perplexity = 1.0\n",
            "Lambdas = (0.1, 0.5, 0.4): Perplexity = 1.0\n",
            "Lambdas = (0.1, 0.6, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.1, 0.7): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.2, 0.6): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.3, 0.5): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.4, 0.4): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.5, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.6, 0.2): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.1, 0.6): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.2, 0.5): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.3, 0.4): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.4, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.5, 0.2): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.6, 0.1): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.1, 0.5): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.2, 0.4): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.3, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.4, 0.2): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.5, 0.1): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.6, 0.0): Perplexity = 1.0\n",
            "Lambdas = (0.5, 0.1, 0.4): Perplexity = 1.0\n",
            "Lambdas = (0.5, 0.2, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.5, 0.3, 0.2): Perplexity = 1.0\n",
            "Lambdas = (0.5, 0.4, 0.1): Perplexity = 1.0\n",
            "Lambdas = (0.5, 0.5, 0.0): Perplexity = 1.0\n",
            "Lambdas = (0.6, 0.1, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.6, 0.2, 0.2): Perplexity = 1.0\n",
            "Lambdas = (0.6, 0.3, 0.1): Perplexity = 1.0\n",
            "Lambdas = (0.6, 0.4, 0.0): Perplexity = 1.0\n",
            "Char LM\n",
            "Evaluating different k values...\n",
            "k = 0.2: Perplexity = 5.000000000000003\n",
            "k = 0.4: Perplexity = 2.5000000000000004\n",
            "k = 0.6: Perplexity = 1.6666666666666667\n",
            "k = 0.8: Perplexity = 1.25\n",
            "k = 1.0: Perplexity = 1.0\n",
            "Searching best lambda...\n",
            "Lambdas = (0.1, 0.1, 0.8): Perplexity = 1.0\n",
            "Lambdas = (0.1, 0.2, 0.7): Perplexity = 1.0\n",
            "Lambdas = (0.1, 0.3, 0.6): Perplexity = 1.0\n",
            "Lambdas = (0.1, 0.4, 0.5): Perplexity = 1.0\n",
            "Lambdas = (0.1, 0.5, 0.4): Perplexity = 1.0\n",
            "Lambdas = (0.1, 0.6, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.1, 0.7): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.2, 0.6): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.3, 0.5): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.4, 0.4): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.5, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.2, 0.6, 0.2): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.1, 0.6): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.2, 0.5): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.3, 0.4): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.4, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.5, 0.2): Perplexity = 1.0\n",
            "Lambdas = (0.3, 0.6, 0.1): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.1, 0.5): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.2, 0.4): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.3, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.4, 0.2): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.5, 0.1): Perplexity = 1.0\n",
            "Lambdas = (0.4, 0.6, 0.0): Perplexity = 1.0\n",
            "Lambdas = (0.5, 0.1, 0.4): Perplexity = 1.0\n",
            "Lambdas = (0.5, 0.2, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.5, 0.3, 0.2): Perplexity = 1.0\n",
            "Lambdas = (0.5, 0.4, 0.1): Perplexity = 1.0\n",
            "Lambdas = (0.5, 0.5, 0.0): Perplexity = 1.0\n",
            "Lambdas = (0.6, 0.1, 0.3): Perplexity = 1.0\n",
            "Lambdas = (0.6, 0.2, 0.2): Perplexity = 1.0\n",
            "Lambdas = (0.6, 0.3, 0.1): Perplexity = 1.0\n",
            "Lambdas = (0.6, 0.4, 0.0): Perplexity = 1.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class NGramLM:\n",
        "    \"\"\"N-gram language model.\"\"\"\n",
        "\n",
        "    def __init__(self, bos_token, eos_token, tokenizer, ngram_size):\n",
        "        \"\"\"\n",
        "        Initialize the NGram Language Model.\n",
        "        \"\"\"\n",
        "        self.ngram_count = {}\n",
        "        for i in range(ngram_size):\n",
        "            self.ngram_count[i] = defaultdict(int)\n",
        "\n",
        "        self.ngram_size = ngram_size\n",
        "        self.vocab_sum = None  # Could be useful in linear interpolation\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "        self.unigram_count = defaultdict(int)\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def get_ngram_counts(self, reviews):\n",
        "        \"\"\"\n",
        "        Store counts in self.ngram_count.\n",
        "        \"\"\"\n",
        "        processed_reviews = []\n",
        "\n",
        "        for review in reviews:\n",
        "            tokens = self.tokenizer(review.lower())\n",
        "\n",
        "            #  Add correct BOS padding\n",
        "            bos_padding = [self.bos_token] * (self.ngram_size - 1)\n",
        "            tokens = bos_padding + tokens + [self.eos_token]\n",
        "\n",
        "            processed_reviews.append(tokens)\n",
        "\n",
        "        #  Step 1: Count unigrams first\n",
        "        unigram_temp = defaultdict(int)\n",
        "        for review in processed_reviews:\n",
        "            for token in review:\n",
        "                unigram_temp[token] += 1\n",
        "\n",
        "        #  Step 2: Replace low-frequency words with \"UNK\"\n",
        "        for review in processed_reviews:\n",
        "            for i in range(len(review)):\n",
        "                if unigram_temp[review[i]] < 2:\n",
        "                    review[i] = \"UNK\"\n",
        "\n",
        "        #  Step 3: Update final unigram counts\n",
        "        self.unigram_count.clear()\n",
        "        for review in processed_reviews:\n",
        "            for token in review:\n",
        "                self.unigram_count[token] += 1\n",
        "\n",
        "        #  Step 4: Count n-grams properly\n",
        "        for review in processed_reviews:\n",
        "            for n in range(1, self.ngram_size + 1):\n",
        "                for i in range(len(review) - n + 1):\n",
        "                    ngram = tuple(review[i:i + n])\n",
        "                    self.ngram_count[n - 1][ngram] += 1\n",
        "\n",
        "        #  Step 5: Compute vocabulary size\n",
        "        self.vocab_size = len(self.unigram_count) - (1 if self.bos_token in self.unigram_count else 0)\n",
        "\n",
        "    def add_k_smooth_prob(self, n_minus1_gram, unigram, k):\n",
        "        \"\"\"\n",
        "        Compute probability using Add-K smoothing.\n",
        "        \"\"\"\n",
        "        probability = 0\n",
        "        ngram = n_minus1_gram + (unigram,)\n",
        "        prefix_count = self.ngram_count[len(n_minus1_gram)].get(n_minus1_gram, 0)\n",
        "        ngram_count = self.ngram_count[len(n_minus1_gram)].get(ngram, 0)\n",
        "\n",
        "        #  Prevent division by zero\n",
        "        probability = (ngram_count + k) / (prefix_count + k * self.vocab_size + 1)\n",
        "        return probability\n",
        "\n",
        "    def linear_interp_prob(self, ngram_prefix, next_token, lambdas):\n",
        "        \"\"\"\n",
        "        Compute probability using Linear Interpolation Smoothing.\n",
        "        \"\"\"\n",
        "        total_prob = 0\n",
        "\n",
        "        #  Prevent division by zero\n",
        "        unigram_prob = (self.unigram_count.get(next_token, 0) + 1) / (self.vocab_size + 1)\n",
        "        total_prob += lambdas[0] * unigram_prob\n",
        "\n",
        "        if len(ngram_prefix) >= 1:\n",
        "            bigram_count = self.ngram_count[1].get((ngram_prefix[-1], next_token), 0)\n",
        "            bigram_prefix_count = self.unigram_count.get(ngram_prefix[-1], 0)\n",
        "            bigram_prob = (bigram_count + 1) / (bigram_prefix_count + self.vocab_size + 1)\n",
        "            total_prob += lambdas[1] * bigram_prob\n",
        "\n",
        "        if len(ngram_prefix) >= 2:\n",
        "            trigram_count = self.ngram_count[2].get((ngram_prefix[-2], ngram_prefix[-1], next_token), 0)\n",
        "            trigram_prefix_count = self.ngram_count[1].get((ngram_prefix[-2], ngram_prefix[-1]), 0)\n",
        "            trigram_prob = (trigram_count + 1) / (trigram_prefix_count + self.vocab_size + 1)\n",
        "            total_prob += lambdas[2] * trigram_prob\n",
        "\n",
        "        return total_prob\n",
        "\n",
        "    def get_probability(self, ngram_prefix, next_token, smoothing_args):\n",
        "        method = smoothing_args.get('method', 'add_k')\n",
        "\n",
        "        if method == 'add_k':\n",
        "            k = smoothing_args.get('k', 0.5)\n",
        "            return self.add_k_smooth_prob(ngram_prefix, next_token, k)\n",
        "        elif method == 'linear':\n",
        "            lambdas = smoothing_args.get('lambdas', [0.6, 0.2, 0.2])\n",
        "            return self.linear_interp_prob(ngram_prefix, next_token, lambdas)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown smoothing method. Use 'add_k' or 'linear'.\")\n",
        "\n",
        "    def get_perplexity(self, text, smoothing_args):\n",
        "        tokens = self.tokenizer(text.lower())\n",
        "        bos_padding = [self.bos_token] * (self.ngram_size - 1)\n",
        "        tokens = bos_padding + tokens + [self.eos_token]\n",
        "        log_prob_sum = 0\n",
        "        N = len(tokens) - (self.ngram_size - 1)\n",
        "\n",
        "        for i in range(self.ngram_size - 1, len(tokens)):\n",
        "            ngram_prefix = tuple(tokens[i - (self.ngram_size - 1):i])\n",
        "            next_token = tokens[i]\n",
        "            prob = self.get_probability(ngram_prefix, next_token, smoothing_args)\n",
        "\n",
        "            if prob > 0:\n",
        "                log_prob_sum += np.log(prob)\n",
        "            else:\n",
        "                log_prob_sum += np.log(1e-10)\n",
        "        avg_log_prob = log_prob_sum / N\n",
        "        perplexity = np.exp(-avg_log_prob)\n",
        "        return perplexity\n",
        "\n",
        "    def search_k(self, dev_text):\n",
        "        k_values = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "        best_k = None\n",
        "        min_perplexity = float('inf')\n",
        "        print(\"Evaluating different k values...\")\n",
        "\n",
        "        for k in k_values:\n",
        "            smoothing_args = {'method': 'add_k', 'k': k}\n",
        "            total_perplexity = 0\n",
        "            num_sentences = len(dev_text)\n",
        "\n",
        "            for sentence in dev_text:\n",
        "                total_perplexity += self.get_perplexity(sentence, smoothing_args)\n",
        "            avg_perplexity = total_perplexity / num_sentences\n",
        "            print(f\"k = {k}: Perplexity = {avg_perplexity}\")\n",
        "\n",
        "            if avg_perplexity < min_perplexity:\n",
        "                min_perplexity = avg_perplexity\n",
        "                best_k = k\n",
        "        return best_k\n",
        "\n",
        "    def search_lambda(self, dev_text):\n",
        "        lambda_options = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "        best_lambdas = None\n",
        "        min_perplexity = float('inf')\n",
        "        lambda_combinations = [\n",
        "            (l1, l2, round(1 - l1 - l2, 2))\n",
        "            for l1, l2 in itertools.product(lambda_options, repeat=2)\n",
        "            if l1 + l2 <= 1\n",
        "        ]\n",
        "        print(\"Searching best lambda...\")\n",
        "        for lambdas in lambda_combinations:\n",
        "            smoothing_args = {'method': 'linear', 'lambdas': lambdas}\n",
        "            total_perplexity = np.mean([self.get_perplexity(sent, smoothing_args) for sent in dev_text])\n",
        "\n",
        "            print(f\"Lambdas = {lambdas}: Perplexity = {total_perplexity}\")\n",
        "            if total_perplexity < min_perplexity:\n",
        "                min_perplexity = total_perplexity\n",
        "                best_lambdas = lambdas\n",
        "        return best_lambdas\n",
        "\n",
        "# Example Test\n",
        "dev_text = [\"This is a validation sentence.\", \"The model should predict well.\", \"Perplexity should be low.\"]\n",
        "word_lm = NGramLM(\"<s>\", \"</s>\", str.split, 3)\n",
        "char_lm = NGramLM(\"<s>\", \"</s>\", list, 4)\n",
        "print(\"Word LM\")\n",
        "best_k_word = word_lm.search_k(dev_text)\n",
        "best_lambda_word = word_lm.search_lambda(dev_text)\n",
        "print(\"Char LM\")\n",
        "best_k_char = char_lm.search_k(dev_text)\n",
        "best_lambda_char = char_lm.search_lambda(dev_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9616083",
      "metadata": {
        "id": "d9616083"
      },
      "source": [
        "### C.1.7 Generate reviews [5 points]\n",
        "Finally, you can automatically generate text using your language models. **Fill in** the function ```generate_text``` to generate a sentence based on an input prompt. To generate the text, you need to find the distribution of the next word given previous two words (or next character given the previous three characters). Then you need to **randomly** sample the next word/character based on the distribution (i.e., do **NOT** use greedy decoding, which is deterministically choosing the most probable unigram). After the word/character is sampled, append it to the current text and continue generating the next word/character. You need to repeat this process untill the current sequence **has 15 words/characters** (including prompts) or you **generate the &lt;/s&gt; token**.\n",
        "\n",
        "Note that there exist more advanced methods to generate text from language models such as beam search, top-k sampling, and top-p sampling. You can refer to [this blog](https://huggingface.co/blog/how-to-generate) to get an idea of what they are. In this assignment, you are not required to implement the advanced methods. Simply sampling from the trigram/four-gram distribution is good enough.\n",
        "\n",
        "Begin with the word-level model here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb9d1e55",
      "metadata": {
        "id": "cb9d1e55",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087b0e0e-d274-4dbe-fbb7-1086b60a06d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word LM\n",
            "Prompt: The location\n",
            "Generated Text: The location\n",
            "\n",
            "Prompt: We ate\n",
            "Generated Text: We ate\n",
            "\n",
            "Prompt: I thought\n",
            "Generated Text: I thought\n",
            "\n",
            "Prompt: It had\n",
            "Generated Text: It had\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_text(self, prompt, smoothing_args):\n",
        "    generated = list(prompt)\n",
        "    max_length = 15\n",
        "    print(f\"Prompt: {' '.join(generated)}\")\n",
        "\n",
        "    while len(generated) < max_length:\n",
        "        ngram_prefix = tuple(generated[-(self.ngram_size - 1):])\n",
        "        next_word_probs = {}\n",
        "        for token in self.unigram_count.keys():\n",
        "            if token != self.bos_token:\n",
        "                prob = self.get_probability(ngram_prefix, token, smoothing_args)\n",
        "                if prob > 0:\n",
        "                    next_word_probs[token] = prob\n",
        "        if not next_word_probs:\n",
        "            break\n",
        "        tokens, probs = zip(*next_word_probs.items())\n",
        "        next_token = random.choices(tokens, weights=probs, k=1)[0]\n",
        "\n",
        "        if next_token == self.eos_token:\n",
        "            break\n",
        "        generated.append(next_token)\n",
        "    print(f\"Generated Text: {' '.join(generated)}\\n\")\n",
        "setattr(NGramLM, \"generate_text\", generate_text)\n",
        "\n",
        "# Example Test\n",
        "prompts = [['The', 'location'], ['We', 'ate'], ['I', 'thought'], ['It', 'had']]\n",
        "smoothing_args1 = {'method': 'add_k', 'k': 0.5}\n",
        "print(\"Word LM\")\n",
        "for prompt in prompts:\n",
        "    word_lm.generate_text(prompt, smoothing_args1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b7b51a",
      "metadata": {
        "id": "49b7b51a"
      },
      "source": [
        "Now test the character-level model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0774fc2f",
      "metadata": {
        "id": "0774fc2f",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e32c934-49c1-499d-dbe8-590ce709d1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char LM\n",
            "Prompt: T h e   s t o r e\n",
            "Generated Text: T h e   s t o r e\n",
            "\n",
            "Char LM\n",
            "Prompt: W e   a t e\n",
            "Generated Text: W e   a t e\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompts = ['The store', 'We ate']\n",
        "for prompt in prompts:\n",
        "    print(\"Char LM\")\n",
        "    prompt_tokenized = char_tokenizer(prompt)\n",
        "    char_lm.generate_text(prompt_tokenized, smoothing_args1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "swiss-dining",
      "metadata": {
        "id": "swiss-dining"
      },
      "source": [
        "### C.1.8 Predict the class of a review [4 points]\n",
        "The Yelp review dataset contains several columns specifying attributes of each review:\n",
        "- ```stars``` indicates on a scale from 1 to 5 how many stars the reviewer gave in the review\n",
        "- ```cool```, ```useful```, and ```funny``` all indicate how many users rated the review as cool, useful or funny\n",
        "\n",
        "Using the same techniques developed above, this section requires you to do the following:\n",
        "- Fill in the function ```load_new_data``` to generate a split in the data. For instance, one class could be reviews with at least one funny rating and the opposing class could be reviews not marked as funny by anyone. In this function, use the techniques at the beginning of the notebook to split the overall dataset into two classes of your choice\n",
        "- Fill in the function ```predict_class``` to predict the class to which the review in ```test_review.txt``` belongs. This function will take in two word-level language models (each trained on the training data for one class) and compute the probability of the review generated by each language model. I.e., $p(W|LM_1)$ and $p(W|LM_2)$, or equivalently $PPL(W)$ based on $LM_1$ and $LM_2$. Print out the perplexity of each language model and your prediction.\n",
        "\n",
        "The test review used has the following attributes:\n",
        "- ```stars``` : 2\n",
        "- ```useful``` : 3\n",
        "- ```funny``` : 7\n",
        "- ```cool``` : 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aware-craft",
      "metadata": {
        "id": "aware-craft",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c11c401b-0c36-4d40-be70-814a1f06d8e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding best lambda values for Word LM...\n",
            "Searching best lambda...\n",
            "Lambdas = (0.1, 0.1, 0.8): Perplexity = 2709.6141272781924\n",
            "Lambdas = (0.1, 0.2, 0.7): Perplexity = 2241.2141745937893\n",
            "Lambdas = (0.1, 0.3, 0.6): Perplexity = 2037.5233246516361\n",
            "Lambdas = (0.1, 0.4, 0.5): Perplexity = 1919.654327721313\n",
            "Lambdas = (0.1, 0.5, 0.4): Perplexity = 1841.4605085950789\n",
            "Lambdas = (0.1, 0.6, 0.3): Perplexity = 1785.191440602496\n",
            "Lambdas = (0.2, 0.1, 0.7): Perplexity = 1959.4417292725914\n",
            "Lambdas = (0.2, 0.2, 0.6): Perplexity = 1635.0371230473554\n",
            "Lambdas = (0.2, 0.3, 0.5): Perplexity = 1485.267817612301\n",
            "Lambdas = (0.2, 0.4, 0.4): Perplexity = 1396.036036808305\n",
            "Lambdas = (0.2, 0.5, 0.3): Perplexity = 1335.7886127230472\n",
            "Lambdas = (0.2, 0.6, 0.2): Perplexity = 1291.9188993898329\n",
            "Lambdas = (0.3, 0.1, 0.6): Perplexity = 1603.0764480535083\n",
            "Lambdas = (0.3, 0.2, 0.5): Perplexity = 1352.1879152544252\n",
            "Lambdas = (0.3, 0.3, 0.4): Perplexity = 1231.2367696667939\n",
            "Lambdas = (0.3, 0.4, 0.3): Perplexity = 1157.5269311184131\n",
            "Lambdas = (0.3, 0.5, 0.2): Perplexity = 1107.044261097056\n",
            "Lambdas = (0.3, 0.6, 0.1): Perplexity = 1069.9184151884224\n",
            "Lambdas = (0.4, 0.1, 0.5): Perplexity = 1382.0226804488302\n",
            "Lambdas = (0.4, 0.2, 0.4): Perplexity = 1177.3794791318644\n",
            "Lambdas = (0.4, 0.3, 0.3): Perplexity = 1075.3332841111624\n",
            "Lambdas = (0.4, 0.4, 0.2): Perplexity = 1011.9869056599546\n",
            "Lambdas = (0.4, 0.5, 0.1): Perplexity = 968.0795933352772\n",
            "Lambdas = (0.4, 0.6, 0.0): Perplexity = 935.513926748261\n",
            "Lambdas = (0.5, 0.1, 0.4): Perplexity = 1227.3495581130155\n",
            "Lambdas = (0.5, 0.2, 0.3): Perplexity = 1054.8667495565212\n",
            "Lambdas = (0.5, 0.3, 0.2): Perplexity = 966.4526217160715\n",
            "Lambdas = (0.5, 0.4, 0.1): Perplexity = 910.7082882492609\n",
            "Lambdas = (0.5, 0.5, 0.0): Perplexity = 871.6700259362804\n",
            "Lambdas = (0.6, 0.1, 0.3): Perplexity = 1111.28224495899\n",
            "Lambdas = (0.6, 0.2, 0.2): Perplexity = 962.5743525851212\n",
            "Lambdas = (0.6, 0.3, 0.1): Perplexity = 884.5581619392323\n",
            "Lambdas = (0.6, 0.4, 0.0): Perplexity = 834.7051081139981\n",
            "Perplexity for Class 1 (Funny Reviews): 560.8353847637983\n",
            "Perplexity for Class 2 (Not Funny Reviews): 637.0762685141615\n",
            "Predicted Class: Class 1 (Funny)\n"
          ]
        }
      ],
      "source": [
        "def load_new_data(df):\n",
        "    class1_data = df[df['funny'] > 0]['text'].tolist()\n",
        "    class2_data = df[df['funny'] == 0]['text'].tolist()\n",
        "    return class1_data, class2_data\n",
        "\n",
        "def predict_class(test_review_file, class1_lm, class2_lm, smoothing_args):\n",
        "    with open(test_review_file, 'r') as file:\n",
        "        test_review = file.read().strip()\n",
        "    perplexity_class1 = class1_lm.get_perplexity(test_review, smoothing_args)\n",
        "    perplexity_class2 = class2_lm.get_perplexity(test_review, smoothing_args)\n",
        "    print(f\"Perplexity for Class 1 (Funny Reviews): {perplexity_class1}\")\n",
        "    print(f\"Perplexity for Class 2 (Not Funny Reviews): {perplexity_class2}\")\n",
        "    predicted_class = \"Class 1 (Funny)\" if perplexity_class1 < perplexity_class2 else \"Class 2 (Not Funny)\"\n",
        "    print(f\"Predicted Class: {predicted_class}\")\n",
        "\n",
        "filename = 'yelp.csv'\n",
        "df = pd.read_csv(filename)\n",
        "class1_data, class2_data = load_new_data(df)\n",
        "class1_lm = NGramLM(\"<s>\", \"</s>\", word_tokenize, 3)\n",
        "class1_lm.get_ngram_counts(class1_data)\n",
        "class2_lm = NGramLM(\"<s>\", \"</s>\", word_tokenize, 3)\n",
        "class2_lm.get_ngram_counts(class2_data)\n",
        "print(\"Finding best lambda values for Word LM...\")\n",
        "word_lambda = class1_lm.search_lambda(class1_data + class2_data)\n",
        "smoothing_args = {'method': 'linear', 'lambdas': word_lambda}\n",
        "predict_class('test_review.txt', class1_lm, class2_lm, smoothing_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regular-poison",
      "metadata": {
        "id": "regular-poison"
      },
      "source": [
        "## C.2 Naive Bayes for Text Classification [26 points]\n",
        "In this problem, you will build naive bayes classifiers to do text classification. You will use the clickbait headlines dataset, which contains examples of legitimate news headlines and clickbait news headlines. The original dataset can be found in [this GitHub repository](https://github.com/bhargaviparanjape/clickbait) and [this paper](https://arxiv.org/abs/1610.09786).\n",
        "### C.2.1 Load dataset [4 points]\n",
        "To get started, **fill in** the function ```load_headlines``` to load the clickbait dataset into pandas dataframes. The file ```clickbait_data.csv``` contains a partially processed subset of the data. It contains two columns: (1) ```is_clickbait``` is 1 when the row contains a clickbait headline and 0 when it doesn't and (2) ```text```, which contains the headline itself.\n",
        "\n",
        "To get started, **fill in** the function ```load_headlines``` to load the clickbait dataset into a pandas dataframe. To do this, you will need to do the following:\n",
        "\n",
        "1. Read in the ```text``` and ```is_clickbait``` columns.\n",
        "2. Rename the ```is_clickbait``` column to ```label```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "realistic-execution",
      "metadata": {
        "id": "realistic-execution",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "0ca88bf1-8ecb-4045-fe33-18b06cf5ae7a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      label                                               text\n",
              "4896      1  Here's How To Wrap A Present Like An Actual Adult\n",
              "4782      1  58 French Villages That Should Be On Your Buck...\n",
              "1496      1  We Need To Talk About Gigi Hadid For Like 10 S...\n",
              "1957      1  Rihanna Released A New Song But The Internet C...\n",
              "9171      0                              300 Laid Off at Sears"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b9504345-ca88-429f-a097-fcd7ee30d972\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4896</th>\n",
              "      <td>1</td>\n",
              "      <td>Here's How To Wrap A Present Like An Actual Adult</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4782</th>\n",
              "      <td>1</td>\n",
              "      <td>58 French Villages That Should Be On Your Buck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>1</td>\n",
              "      <td>We Need To Talk About Gigi Hadid For Like 10 S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1957</th>\n",
              "      <td>1</td>\n",
              "      <td>Rihanna Released A New Song But The Internet C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9171</th>\n",
              "      <td>0</td>\n",
              "      <td>300 Laid Off at Sears</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9504345-ca88-429f-a097-fcd7ee30d972')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b9504345-ca88-429f-a097-fcd7ee30d972 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b9504345-ca88-429f-a097-fcd7ee30d972');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-16d48def-ef96-49df-bae9-df8b784cf1b2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-16d48def-ef96-49df-bae9-df8b784cf1b2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-16d48def-ef96-49df-bae9-df8b784cf1b2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(test\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"58 French Villages That Should Be On Your Bucket List\",\n          \"300 Laid Off at Sears\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      label                                               text\n",
              "6252      0  'Denmark will be attacked' says one expert, 'D...\n",
              "4684      1  14 Things You Never Noticed During The Opening...\n",
              "1731      1  This Is What It's Actually Like To Live As An ...\n",
              "4742      1  This Is What Happens When A Lesbian Runs A Gay...\n",
              "4521      1  25 Gifts For The Ultimate Wine Lover In Your Life"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7ff3389-427c-4848-922f-9a0f7c97131b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6252</th>\n",
              "      <td>0</td>\n",
              "      <td>'Denmark will be attacked' says one expert, 'D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4684</th>\n",
              "      <td>1</td>\n",
              "      <td>14 Things You Never Noticed During The Opening...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1731</th>\n",
              "      <td>1</td>\n",
              "      <td>This Is What It's Actually Like To Live As An ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4742</th>\n",
              "      <td>1</td>\n",
              "      <td>This Is What Happens When A Lesbian Runs A Gay...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4521</th>\n",
              "      <td>1</td>\n",
              "      <td>25 Gifts For The Ultimate Wine Lover In Your Life</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7ff3389-427c-4848-922f-9a0f7c97131b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7ff3389-427c-4848-922f-9a0f7c97131b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7ff3389-427c-4848-922f-9a0f7c97131b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9de5e510-cbbd-491e-98cc-1fd3a6a13b01\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9de5e510-cbbd-491e-98cc-1fd3a6a13b01')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9de5e510-cbbd-491e-98cc-1fd3a6a13b01 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(test\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"14 Things You Never Noticed During The Opening Number From \\\"Beauty And The Beast\\\"\",\n          \"25 Gifts For The Ultimate Wine Lover In Your Life\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def load_headlines(filename):\n",
        "    df = pd.read_csv(filename, usecols=[\"is_clickbait\", \"text\"])\n",
        "    df.rename(columns={\"is_clickbait\": \"label\"}, inplace=True)\n",
        "    return df\n",
        "all_data = load_headlines('clickbait_data.csv')\n",
        "train, test = train_test_split(all_data, train_size=0.9, random_state=42)\n",
        "display(train.head())\n",
        "display(test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "earlier-spank",
      "metadata": {
        "id": "earlier-spank"
      },
      "source": [
        "### C.2.2 Dataset statistics [3 points]\n",
        "Before start training classifiers, you need to calculate some basic statistics of the dataset. **Fill in** the function ```get_basic_stats``` to print out the following statistics of the training data:\n",
        "- Average number of tokens per headline\n",
        "- Standard deviation of the number of tokens per headline\n",
        "- Total number of legitimate headlines\n",
        "- Total number of clickbait headlines\n",
        "\n",
        "Note: you can use any tokenization method you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sticky-account",
      "metadata": {
        "id": "sticky-account",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22252190-ef1d-4fef-8a75-f9399ee97d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of tokens per headline: 9.63\n",
            "Standard deviation: 2.96\n",
            "Number of legitimate/clickbait headlines: {1: 4508, 0: 4492}\n"
          ]
        }
      ],
      "source": [
        "def get_basic_stats(df):\n",
        "    token_lengths = df[\"text\"].apply(lambda x: len(word_tokenize(x)))\n",
        "    avg_len = np.mean(token_lengths)\n",
        "    std_len = np.std(token_lengths)\n",
        "    num_articles = df[\"label\"].value_counts().to_dict()\n",
        "    print(f\"Average number of tokens per headline: {avg_len:.2f}\")\n",
        "    print(f\"Standard deviation: {std_len:.2f}\")\n",
        "    print(f\"Number of legitimate/clickbait headlines: {num_articles}\")\n",
        "get_basic_stats(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exterior-rental",
      "metadata": {
        "id": "exterior-rental"
      },
      "source": [
        "### C.2.3 Data processing and ngram calculation [6 points]\n",
        "Now you need to calculate the ngram counts. **Fill in** the function ```fit``` that, given a dataframe of training data, calculates the ngram counts in each category and the prior probability for each category. Concretely, **store** the total occurrence of each ngram in each category in a list called ```self.ngram_count``` so that ```self.ngram_count[0]``` contains $count(w, c_0)$ for all $w$ in the vocabulary, and ```self.ngram_count[1]``` contains $count(w, c_1)$, etc. ```self.ngram_count[i]``` should be an array of shape $(1,|V|)$, where $V$ is the vocabulary (total vocabulary across both classes). **Store** the total occurrence of all ngrams in each category in a list called ```self.total_count``` so that ```self.total_count[0]``` $=\\sum_{w\\in V}count(w, c_0)$, and ```self.total_count[1]``` $=\\sum_{w\\in V}count(w, c_1)$, etc. **Store** the prior probability for each category in ```self.category_prob```. You need to follow these rules when calculating the counts:\n",
        "- convert all letters to lowercase;\n",
        "- include both unigrams and bigrams;\n",
        "- ignore terms that appear in more than 80\\% of the headlines;\n",
        "- ignore terms that appear in less than 3 headlines.\n",
        "\n",
        "Hint: use ```CountVectorizer``` in sklearn and store it as ```self.vectorizer```. You need to use **both legitimate and clickbait headlines** to get the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "personalized-opening",
      "metadata": {
        "id": "personalized-opening",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd17bda8-093e-48a8-a055-8940b85644d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability for each category: [0.4991111111111111, 0.5008888888888889]\n",
            "Length of self.ngram_count: 2\n",
            "Shape of the counts for 1st category: (1, 7208)\n",
            "Number of non-zero terms for 1st category: 4746\n",
            "Maximum count of the 1st category: 1296\n",
            "Minimum count of the 1st category: 0\n",
            "Sum of ngram count for 1st category: 35547\n",
            "Total count for each category: [35547, 56388]\n"
          ]
        }
      ],
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = None\n",
        "        self.ngram_count = []\n",
        "        self.total_count = []\n",
        "        self.category_prob = []\n",
        "\n",
        "    def fit(self, data):\n",
        "        texts = data[\"text\"].str.lower().tolist()\n",
        "        labels = data[\"label\"].tolist()\n",
        "        self.vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.8)\n",
        "        X = self.vectorizer.fit_transform(texts)\n",
        "        X_clickbait = X[np.array(labels) == 1]\n",
        "        X_legit = X[np.array(labels) == 0]\n",
        "        self.ngram_count = [X_legit.sum(axis=0), X_clickbait.sum(axis=0)]\n",
        "        self.total_count = [self.ngram_count[0].sum(), self.ngram_count[1].sum()]\n",
        "        num_samples = len(labels)\n",
        "        self.category_prob = [\n",
        "            sum(1 for label in labels if label == 0) / num_samples,  # P(legit)\n",
        "            sum(1 for label in labels if label == 1) / num_samples   # P(clickbait)\n",
        "        ]\n",
        "naive_bayes = NaiveBayes()\n",
        "naive_bayes.fit(train)\n",
        "print(f\"Probability for each category: {naive_bayes.category_prob}\")\n",
        "print(f\"Length of self.ngram_count: {len(naive_bayes.ngram_count)}\")\n",
        "print(f\"Shape of the counts for 1st category: {naive_bayes.ngram_count[0].shape}\")\n",
        "print(f\"Number of non-zero terms for 1st category: {(naive_bayes.ngram_count[0] > 0).sum()}\")\n",
        "print(f\"Maximum count of the 1st category: {naive_bayes.ngram_count[0].max()}\")\n",
        "print(f\"Minimum count of the 1st category: {naive_bayes.ngram_count[0].min()}\")\n",
        "print(f\"Sum of ngram count for 1st category: {naive_bayes.ngram_count[0].sum()}\")\n",
        "print(f\"Total count for each category: {naive_bayes.total_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blind-decimal",
      "metadata": {
        "id": "blind-decimal"
      },
      "source": [
        "### C.2.4 Calculate posterior probability for a category [4 points]\n",
        "Next, you will use the vectorizer and ngram counts to calculate the posterior probability of a category. In this homework, we have two categories: legitimate and clickbait. **Fill in** the function ```calculate_prob``` that given a list of articles $docs$, a category index $i$, return $\\log\\left(p(c_i)p(d|c_i)\\right)=\\log\\left(p(c_i)\\prod_{x\\in X}p(x|c_i)\\right)$ for each article $d$ in $docs$, where $X$ is the set of unigrams and bigrams in **both** article $d$ and vocabulary $V$.\n",
        "\n",
        "- Use **add-one smoothing** in your calculation.\n",
        "- Simply discard unseen unigrams/bigrams (do not use add-one smoothing to account for them).\n",
        "- Calculate the **sum of logarithms** to avoid issues with underflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brave-determination",
      "metadata": {
        "id": "brave-determination",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e115bb1-5639-49d9-e179-885b7b19d879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability for category 0 (Legit): [-61.981371323938866, -69.61324807840032]\n",
            "Probability for category 1 (Clickbait): [-77.83310517298293, -66.98539561864114]\n"
          ]
        }
      ],
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = None\n",
        "        self.ngram_count = []\n",
        "        self.total_count = []\n",
        "        self.category_prob = []\n",
        "\n",
        "    def fit(self, data):\n",
        "        texts = data[\"text\"].str.lower().tolist()\n",
        "        labels = data[\"label\"].tolist()\n",
        "        self.vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.8)\n",
        "        X = self.vectorizer.fit_transform(texts)\n",
        "        X_clickbait = X[np.array(labels) == 1]\n",
        "        X_legit = X[np.array(labels) == 0]\n",
        "        self.ngram_count = [X_legit.sum(axis=0), X_clickbait.sum(axis=0)]\n",
        "        self.total_count = [self.ngram_count[0].sum(), self.ngram_count[1].sum()]\n",
        "        num_samples = len(labels)\n",
        "        self.category_prob = [\n",
        "            sum(1 for label in labels if label == 0) / num_samples,\n",
        "            sum(1 for label in labels if label == 1) / num_samples\n",
        "        ]\n",
        "\n",
        "    def calculate_prob(self, docs, c_i):\n",
        "        X_docs = self.vectorizer.transform(docs)\n",
        "        ngram_counts = self.ngram_count[c_i]\n",
        "        total_count = self.total_count[c_i]\n",
        "        vocab_size = len(self.vectorizer.vocabulary_)\n",
        "        log_probs = []\n",
        "        log_prior = np.log(self.category_prob[c_i])\n",
        "\n",
        "        for i in range(X_docs.shape[0]):\n",
        "            log_likelihood = 0\n",
        "            for j in range(X_docs.shape[1]):\n",
        "                count_x = X_docs[i, j]\n",
        "                if count_x > 0:\n",
        "                    word_count = ngram_counts[0, j]\n",
        "                    prob_x_given_ci = (word_count + 1) / (total_count + vocab_size)\n",
        "                    log_likelihood += count_x * np.log(prob_x_given_ci)\n",
        "            log_probs.append(log_prior + log_likelihood)\n",
        "        return log_probs\n",
        "naive_bayes = NaiveBayes()\n",
        "naive_bayes.fit(train)\n",
        "test_docs = [\"United Kingdom officially exits the European Union\",\n",
        "             \"How to Lose a Guy in 10 Days\"]\n",
        "prob1 = naive_bayes.calculate_prob(test_docs, 0)\n",
        "prob2 = naive_bayes.calculate_prob(test_docs, 1)\n",
        "print(f\"Probability for category 0 (Legit): {prob1}\")\n",
        "print(f\"Probability for category 1 (Clickbait): {prob2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "occupied-sheep",
      "metadata": {
        "id": "occupied-sheep"
      },
      "source": [
        "### C.2.5 Predict labels for new headlines [2 points]\n",
        "With the posterior probability of each category, you can predict the label for new headlines. **Fill in** the function ```predict``` that, given a list of headlines, returns the predicted categories of the headlines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "opposed-territory",
      "metadata": {
        "id": "opposed-territory",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c82cc6b-aea3-4ba2-9b26-0fd953ee07e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: [0, 1]\n"
          ]
        }
      ],
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = None\n",
        "        self.ngram_count = []\n",
        "        self.total_count = []\n",
        "        self.category_prob = []\n",
        "\n",
        "    def fit(self, data):\n",
        "        texts = data[\"text\"].str.lower().tolist()\n",
        "        labels = data[\"label\"].tolist()\n",
        "        self.vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.8)\n",
        "        X = self.vectorizer.fit_transform(texts)\n",
        "        X_clickbait = X[np.array(labels) == 1]\n",
        "        X_legit = X[np.array(labels) == 0]\n",
        "        self.ngram_count = [X_legit.sum(axis=0), X_clickbait.sum(axis=0)]\n",
        "        self.total_count = [self.ngram_count[0].sum(), self.ngram_count[1].sum()]\n",
        "        num_samples = len(labels)\n",
        "        self.category_prob = [\n",
        "            sum(1 for label in labels if label == 0) / num_samples,\n",
        "            sum(1 for label in labels if label == 1) / num_samples\n",
        "        ]\n",
        "\n",
        "    def calculate_prob(self, docs, c_i):\n",
        "        X_docs = self.vectorizer.transform(docs)\n",
        "        ngram_counts = self.ngram_count[c_i]\n",
        "        total_count = self.total_count[c_i]\n",
        "        vocab_size = len(self.vectorizer.vocabulary_)\n",
        "        log_probs = []\n",
        "        log_prior = np.log(self.category_prob[c_i])\n",
        "        for i in range(X_docs.shape[0]):\n",
        "            log_likelihood = 0\n",
        "            for j in range(X_docs.shape[1]):\n",
        "                count_x = X_docs[i, j]\n",
        "                if count_x > 0:\n",
        "                    word_count = ngram_counts[0, j]\n",
        "                    prob_x_given_ci = (word_count + 1) / (total_count + vocab_size)\n",
        "                    log_likelihood += count_x * np.log(prob_x_given_ci)\n",
        "            log_probs.append(log_prior + log_likelihood)\n",
        "        return log_probs\n",
        "\n",
        "    def predict(self, docs):\n",
        "        prob_legit = self.calculate_prob(docs, 0)\n",
        "        prob_clickbait = self.calculate_prob(docs, 1)\n",
        "        predictions = [1 if prob_clickbait[i] > prob_legit[i] else 0 for i in range(len(docs))]\n",
        "        return predictions\n",
        "naive_bayes = NaiveBayes()\n",
        "naive_bayes.fit(train)\n",
        "test_docs = [\"United Kingdom officially exits the European Union\",\n",
        "             \"How to Lose a Guy in 10 Days\"]\n",
        "preds = naive_bayes.predict(test_docs)\n",
        "print(f\"Prediction: {preds}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bronze-semester",
      "metadata": {
        "id": "bronze-semester"
      },
      "source": [
        "### C.2.6 Calculate evaluation metrics [5 points]\n",
        "To evaluate a classifier, you need to calculate some evaluation metrics. **Fill in** the function ```evaluate``` that, given a list of predictions and a list of true labels, returns the accuracy, macro f1-score, and micro f1-score. You can **NOT** use functions in sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "level-casino",
      "metadata": {
        "id": "level-casino",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de6bd22-a001-41a7-8ffe-b637acd9eba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.71\n",
            "Macro F1-score: 0.71\n",
            "Micro F1-score: 0.71\n"
          ]
        }
      ],
      "source": [
        "def evaluate(predictions, labels):\n",
        "    correct = sum(1 for p, l in zip(predictions, labels) if p == l)\n",
        "    accuracy = correct / len(labels)\n",
        "    class_counts = {0: {\"TP\": 0, \"FP\": 0, \"FN\": 0}, 1: {\"TP\": 0, \"FP\": 0, \"FN\": 0}}\n",
        "    for p, l in zip(predictions, labels):\n",
        "        if p == l:\n",
        "            class_counts[l][\"TP\"] += 1\n",
        "        else:\n",
        "            class_counts[p][\"FP\"] += 1\n",
        "            class_counts[l][\"FN\"] += 1\n",
        "    macro_f1 = 0\n",
        "    for cls in [0, 1]:\n",
        "        TP, FP, FN = class_counts[cls][\"TP\"], class_counts[cls][\"FP\"], class_counts[cls][\"FN\"]\n",
        "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        macro_f1 += f1\n",
        "    macro_f1 /= 2\n",
        "    total_TP = class_counts[0][\"TP\"] + class_counts[1][\"TP\"]\n",
        "    total_FP = class_counts[0][\"FP\"] + class_counts[1][\"FP\"]\n",
        "    total_FN = class_counts[0][\"FN\"] + class_counts[1][\"FN\"]\n",
        "    micro_f1 = (2 * total_TP) / (2 * total_TP + total_FP + total_FN) if (2 * total_TP + total_FP + total_FN) > 0 else 0\n",
        "    return accuracy, macro_f1, micro_f1\n",
        "predictions = [1, 1, 0, 1, 0, 0, 1]\n",
        "labels = [1, 0, 0, 1, 0, 1, 1]\n",
        "accuracy, mac_f1, mic_f1 = evaluate(predictions, labels)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Macro F1-score: {mac_f1:.2f}\")\n",
        "print(f\"Micro F1-score: {mic_f1:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "warming-corner",
      "metadata": {
        "id": "warming-corner"
      },
      "source": [
        "### C.2.7 Test classifier on test data [2 points]\n",
        "Finally, you are ready to evaluate your classifier on the test data! Run the following cell to make predictions and print out performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "amended-angle",
      "metadata": {
        "id": "amended-angle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc6afc7-9b4d-4252-f876-8b42fb7f5933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.964\n",
            "Macro f1: 0.9639994239907839\n",
            "Micro f1: 0.964\n"
          ]
        }
      ],
      "source": [
        "predictions = naive_bayes.predict(test.text.tolist())\n",
        "labels = test.label.tolist()\n",
        "accuracy, mac_f1, mic_f1 = evaluate(predictions, labels)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Macro f1: {mac_f1}\")\n",
        "print(f\"Micro f1: {mic_f1}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}